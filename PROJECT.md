# module objectives

● Students will learn safety and security issues of AI systems.
● Students will learn how to conduct attacks on AI systems trained on
(mostly) image and (sometimes) text datasets.
● Students will learn state-of-the-art AI system evaluation methods for
robustness, backdoor-freeness, fairness, privacy and interpretability.
● Students will have hand-on experience on developing software toolkits for
evaluating AI systems.
● Students will learn how to improve AI safety and security through robust
training, backdoor removal, fairness and privacy enhancement.

## Example of past project that did well

project in 2024 run did so well they publish paper

idea is simple and intuitive:

in LLM, if they are telling the truth, there will be one clear layer that's high in confidence

if tell lie, no layers will be high in confidence.

That's the idea they wanted to test.

see the [full paper](crow-paper.md)

## Criteria

Group project (ideally 3 in a group, minimum 2, maximum 4) with 40% weightage

You are encouraged to find your own group and we will help if you have trouble identifying a group.

Initial project proposal is due by the end of Week 4. High-level feedback will be provided.

Project is to be presented in Week 10 class, and the final report is due at the end of Week 10.

You are to propose your own project of the following nature.

- Case study: Systematic evaluation of a real-world AI system/app/Website in terms of one or more dimensions of AI safety (e.g., robustness, backdoor-freeness, fairness, privacy, harmfulness, interpretability and so on)
- Research: Proposal and implementation of novel techniques for evaluating/improving certain dimension of AI safety.

It must be non-trivial.

Enough for a team of 3
