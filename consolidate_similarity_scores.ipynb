{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate Similarity Scores Across Multiple Runs\n",
    "\n",
    "This notebook extracts and consolidates similarity scores from multiple run_summary.json files stored in Google Drive.\n",
    "\n",
    "**Key Points:**\n",
    "- Similarity scores are at the **instance level** (not instance-run level)\n",
    "- They compare multiple runs of the same instance\n",
    "- Located in: `multi_run_info.consistency_metrics.instance_reports`\n",
    "\n",
    "**Setup:**\n",
    "- Designed for Google Colab\n",
    "- Mounts Google Drive to access data files\n",
    "- Outputs saved to: `/content/drive/MyDrive/aisysevalteam-data/consistency_extraction/`\n",
    "\n",
    "**Quick Start:**\n",
    "1. Run all cells in order\n",
    "2. Adjust `model_name` in Configuration cell for different models\n",
    "3. CSV files and visualizations will be saved to the `consistency_extraction` subdirectory in Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**Update these variables to match your Google Drive structure:**\n",
    "- `file_path`: Base directory in Google Drive where results are stored\n",
    "- `model_name`: Specific model directory to analyze (e.g., 'google/gemini2.5pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "file_path = '/content/drive/MyDrive/aisysevalteam-data/'\n",
    "# @TODO change this to extract per model\n",
    "# available are: openai-gpt-5-nano, google-gemini2.5pro, qwen3-coder, openai/gpt-5\n",
    "# model_name = 'google-gemini2.5pro'\n",
    "model_name = 'openai/gpt-5'\n",
    "\n",
    "# Construct base directory path\n",
    "base_dir = Path(file_path) / model_name\n",
    "\n",
    "# Alternative: Search all models in the base directory\n",
    "# base_dir = Path(file_path)\n",
    "\n",
    "print(f\"Base data path: {file_path}\")\n",
    "print(f\"Model name: {model_name}\")\n",
    "print(f\"Searching in: {base_dir}\")\n",
    "print(f\"Directory exists: {base_dir.exists()}\")\n",
    "\n",
    "# If directory doesn't exist, list what's available\n",
    "if not base_dir.exists():\n",
    "    print(f\"\\nâš ï¸ Directory not found!\")\n",
    "    print(f\"\\nAvailable directories in {file_path}:\")\n",
    "    base_path = Path(file_path)\n",
    "    if base_path.exists():\n",
    "        for item in sorted(base_path.iterdir()):\n",
    "            if item.is_dir():\n",
    "                print(f\"  - {item.name}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Base path {file_path} does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find All run_summary.json Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all run_summary.json files\n",
    "run_summary_files = list(base_dir.glob(\"**/run_summary.json\"))\n",
    "\n",
    "print(f\"Found {len(run_summary_files)} run_summary.json files:\\n\")\n",
    "for f in sorted(run_summary_files):\n",
    "    print(f\"  {f.relative_to(base_dir.parent)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Similarity Scores by Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all data by instance_id\n",
    "# Structure: {instance_id: [record1, record2, ...]}\n",
    "instances_data = defaultdict(list)\n",
    "\n",
    "for summary_file in run_summary_files:\n",
    "    try:\n",
    "        with open(summary_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Extract run metadata\n",
    "        run_info = data.get('run_info', {})\n",
    "        model_name_from_file = run_info.get('model_name', 'unknown')\n",
    "        timestamp = run_info.get('timestamp', 'unknown')\n",
    "        num_runs_configured = run_info.get('num_runs_configured', 1)\n",
    "\n",
    "        # Calculate relative path for run_dir\n",
    "        try:\n",
    "            run_dir_rel = str(summary_file.parent.relative_to(base_dir))\n",
    "        except ValueError:\n",
    "            # If relative_to fails, use the full path\n",
    "            run_dir_rel = str(summary_file.parent)\n",
    "\n",
    "        # Extract instance reports from multi_run_info\n",
    "        multi_run_info = data.get('multi_run_info', {})\n",
    "        consistency_metrics = multi_run_info.get('consistency_metrics', {})\n",
    "\n",
    "        # Handle both completed consistency checks and skipped ones\n",
    "        if isinstance(consistency_metrics, dict):\n",
    "            instance_reports = consistency_metrics.get('instance_reports', [])\n",
    "        else:\n",
    "            instance_reports = []\n",
    "\n",
    "        # Process each instance report\n",
    "        for report in instance_reports:\n",
    "            instance_id = report.get('instance_id')\n",
    "            if not instance_id:\n",
    "                continue\n",
    "\n",
    "            # Extract patch consistency metrics\n",
    "            patch_consistency = report.get('patch_consistency', {})\n",
    "            evaluation_consistency = report.get('evaluation_consistency', {})\n",
    "            security_consistency = report.get('security_consistency', {})\n",
    "\n",
    "            # Create record\n",
    "            record = {\n",
    "                'instance_id': instance_id,\n",
    "                'model_name': model_name_from_file,\n",
    "                'timestamp': timestamp,\n",
    "                'run_dir': run_dir_rel,\n",
    "                'num_runs': report.get('num_runs', 1),\n",
    "                'num_runs_configured': num_runs_configured,\n",
    "\n",
    "                # Similarity scores (0.0-1.0)\n",
    "                'avg_ast_similarity': patch_consistency.get('avg_ast_similarity'),\n",
    "                'avg_text_similarity': patch_consistency.get('avg_text_similarity'),\n",
    "                'avg_hybrid_similarity': patch_consistency.get('avg_hybrid_similarity'),\n",
    "\n",
    "                # Other patch metrics\n",
    "                'exact_match_rate': patch_consistency.get('exact_match_rate'),\n",
    "                'num_unique_patches': patch_consistency.get('num_unique_patches'),\n",
    "                'confidence_percent': patch_consistency.get('confidence_percent'),\n",
    "                'normalized_confidence_percent': patch_consistency.get('normalized_confidence_percent'),\n",
    "                'agreement_percent': patch_consistency.get('agreement_percent'),\n",
    "                'line_count_variance': patch_consistency.get('line_count_variance'),\n",
    "\n",
    "                # Consistency scores\n",
    "                'overall_consistency_score': report.get('overall_consistency_score'),\n",
    "                'consistency_grade': report.get('consistency_grade'),\n",
    "\n",
    "                # Component scores\n",
    "                'patch_score': report.get('component_scores', {}).get('patch_score'),\n",
    "                'evaluation_score': report.get('component_scores', {}).get('evaluation_score'),\n",
    "                'security_score': report.get('component_scores', {}).get('security_score'),\n",
    "\n",
    "                # Evaluation consistency\n",
    "                'all_resolved': evaluation_consistency.get('all_resolved'),\n",
    "                'all_failed': evaluation_consistency.get('all_failed'),\n",
    "                'resolution_rate': evaluation_consistency.get('resolution_rate'),\n",
    "                'num_evaluations': evaluation_consistency.get('num_evaluations'),\n",
    "\n",
    "                # Security consistency\n",
    "                'risk_level_mode': security_consistency.get('risk_level_mode'),\n",
    "                'security_score_variance': security_consistency.get('score_variance'),\n",
    "                'num_scans': security_consistency.get('num_scans'),\n",
    "                'avg_security_score': security_consistency.get('avg_score'),\n",
    "            }\n",
    "\n",
    "            instances_data[instance_id].append(record)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {summary_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nExtracted data for {len(instances_data)} unique instances\")\n",
    "print(f\"Total records: {sum(len(records) for records in instances_data.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Consolidated DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data into a list of records\n",
    "all_records = []\n",
    "for instance_id, records in instances_data.items():\n",
    "    all_records.extend(records)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_records)\n",
    "\n",
    "# Sort by instance_id and timestamp\n",
    "df = df.sort_values(['instance_id', 'timestamp'])\n",
    "\n",
    "print(f\"Created DataFrame with {len(df)} rows and {len(df.columns)} columns\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "\n",
    "The extracted dataset includes the following fields:\n",
    "\n",
    "### Metadata\n",
    "- **instance_id**: SWE-bench instance identifier\n",
    "- **model_name**: AI model used\n",
    "- **timestamp**: Run timestamp\n",
    "- **run_dir**: Directory containing the run data\n",
    "- **num_runs**: Actual number of runs performed\n",
    "- **num_runs_configured**: Number of runs configured\n",
    "\n",
    "### Similarity Scores (0.0-1.0, higher = more similar)\n",
    "- **avg_ast_similarity**: Average structural code similarity (AST-based)\n",
    "- **avg_text_similarity**: Average surface-level text similarity\n",
    "- **avg_hybrid_similarity**: Weighted combination of AST and text similarity\n",
    "\n",
    "### Patch Consistency Metrics\n",
    "- **exact_match_rate**: Fraction of patches that are byte-for-byte identical\n",
    "- **num_unique_patches**: Number of distinct patches generated\n",
    "- **confidence_percent**: Main consistency metric (0-100)\n",
    "- **normalized_confidence_percent**: Rescaled confidence metric (0-100)\n",
    "- **agreement_percent**: Percentage of pairs above similarity threshold (0-100)\n",
    "- **line_count_variance**: Variation in patch sizes\n",
    "\n",
    "### Overall Consistency\n",
    "- **overall_consistency_score**: Weighted score combining all metrics (0-100)\n",
    "- **consistency_grade**: EXCELLENT / GOOD / FAIR / POOR\n",
    "\n",
    "### Component Scores (0-100)\n",
    "- **patch_score**: Patch consistency component (40% weight)\n",
    "- **evaluation_score**: Evaluation consistency component (40% weight)\n",
    "- **security_score**: Security consistency component (20% weight)\n",
    "\n",
    "### Evaluation Consistency\n",
    "- **all_resolved**: Whether all runs resolved the issue\n",
    "- **all_failed**: Whether all runs failed the evaluation\n",
    "- **resolution_rate**: Fraction of runs that passed (0.0-1.0)\n",
    "- **num_evaluations**: Number of evaluations performed\n",
    "\n",
    "### Security Consistency\n",
    "- **risk_level_mode**: Most common security risk level (NONE/LOW/MEDIUM/HIGH)\n",
    "- **security_score_variance**: Variance in security scores\n",
    "- **num_scans**: Number of security scans performed\n",
    "- **avg_security_score**: Average security risk score across runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SIMILARITY SCORES SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "similarity_cols = ['avg_ast_similarity', 'avg_text_similarity', 'avg_hybrid_similarity']\n",
    "print(\"\\nSimilarity Scores Statistics (0.0 = very different, 1.0 = identical):\")\n",
    "print(df[similarity_cols].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONSISTENCY GRADES DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "print(df['consistency_grade'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONSISTENCY SCORES BY GRADE\")\n",
    "print(\"=\" * 80)\n",
    "print(df.groupby('consistency_grade')['overall_consistency_score'].agg(['mean', 'min', 'max', 'count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Specific Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show instances with multiple runs across different timestamps\n",
    "instance_counts = df.groupby('instance_id').size().sort_values(ascending=False)\n",
    "print(\"Instances with multiple consistency reports:\")\n",
    "print(instance_counts[instance_counts > 1])\n",
    "\n",
    "# Pick an instance to examine in detail\n",
    "if len(instance_counts) > 0:\n",
    "    example_instance = instance_counts.index[0]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Detailed view of: {example_instance}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    cols_to_show = [\n",
    "        'timestamp', 'num_runs',\n",
    "        'avg_ast_similarity', 'avg_text_similarity', 'avg_hybrid_similarity',\n",
    "        'overall_consistency_score', 'consistency_grade'\n",
    "    ]\n",
    "\n",
    "    display(df[df['instance_id'] == example_instance][cols_to_show])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Consistency Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show instances with EXCELLENT consistency\n",
    "excellent = df[df['consistency_grade'] == 'EXCELLENT']\n",
    "print(f\"Instances with EXCELLENT consistency: {len(excellent)}\")\n",
    "if len(excellent) > 0:\n",
    "    display(excellent[['instance_id', 'timestamp', 'avg_ast_similarity',\n",
    "                       'avg_text_similarity', 'overall_consistency_score']].head(10))\n",
    "\n",
    "# Show instances with POOR consistency\n",
    "poor = df[df['consistency_grade'] == 'POOR']\n",
    "print(f\"\\nInstances with POOR consistency: {len(poor)}\")\n",
    "if len(poor) > 0:\n",
    "    display(poor[['instance_id', 'timestamp', 'avg_ast_similarity',\n",
    "                  'avg_text_similarity', 'overall_consistency_score']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(file_path) / \"consistency_extraction\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {output_dir}\\n\")\n",
    "\n",
    "# Export full dataset\n",
    "output_file = output_dir / f'consolidated_similarity_scores_{model_name.replace(\"/\", \"_\")}.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"âœ… Exported full dataset to: {output_file}\")\n",
    "\n",
    "# Export summary by instance (latest run only)\n",
    "summary_file = output_dir / f'similarity_scores_summary_{model_name.replace(\"/\", \"_\")}.csv'\n",
    "latest_df = df.sort_values('timestamp').groupby('instance_id').last().reset_index()\n",
    "latest_df.to_csv(summary_file, index=False)\n",
    "print(f\"âœ… Exported summary (latest run per instance) to: {summary_file}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Full dataset: {len(df)} records\")\n",
    "print(f\"ðŸ“Š Summary dataset: {len(latest_df)} records (one per instance)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Plot 1: Distribution of similarity scores\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "df['avg_ast_similarity'].hist(bins=20, ax=axes[0], edgecolor='black')\n",
    "axes[0].set_title('AST Similarity Distribution')\n",
    "axes[0].set_xlabel('Similarity Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "df['avg_text_similarity'].hist(bins=20, ax=axes[1], edgecolor='black')\n",
    "axes[1].set_title('Text Similarity Distribution')\n",
    "axes[1].set_xlabel('Similarity Score')\n",
    "\n",
    "df['avg_hybrid_similarity'].hist(bins=20, ax=axes[2], edgecolor='black')\n",
    "axes[2].set_title('Hybrid Similarity Distribution')\n",
    "axes[2].set_xlabel('Similarity Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plot1_file = Path(file_path) / f'similarity_distributions_{model_name.replace(\"/\", \"_\")}.png'\n",
    "plt.savefig(plot1_file, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Consistency scores by grade\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.boxplot(column='overall_consistency_score', by='consistency_grade', ax=ax)\n",
    "plt.title('Consistency Scores by Grade')\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.xlabel('Consistency Grade')\n",
    "plt.ylabel('Overall Consistency Score')\n",
    "plot2_file = Path(file_path) / f'consistency_by_grade_{model_name.replace(\"/\", \"_\")}.png'\n",
    "plt.savefig(plot2_file, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved visualizations:\")\n",
    "print(f\"  - {plot1_file}\")\n",
    "print(f\"  - {plot2_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlation between different metrics\n",
    "correlation_cols = [\n",
    "    'avg_ast_similarity', 'avg_text_similarity', 'avg_hybrid_similarity',\n",
    "    'exact_match_rate', 'confidence_percent', 'overall_consistency_score'\n",
    "]\n",
    "\n",
    "correlation_matrix = df[correlation_cols].corr()\n",
    "\n",
    "# Use the output directory\n",
    "output_dir = Path(file_path) / \"consistency_extraction\"\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Correlation Between Consistency Metrics')\n",
    "plt.tight_layout()\n",
    "heatmap_file = output_dir / f'correlation_heatmap_{model_name.replace(\"/\", \"_\")}.png'\n",
    "plt.savefig(heatmap_file, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Saved {heatmap_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "**Configuration:**\n",
    "- All outputs are saved to Google Drive: `/content/drive/MyDrive/aisysevalteam-data/consistency_extraction/`\n",
    "- Files are named with the model name to avoid conflicts: `consolidated_similarity_scores_google_gemini2.5pro.csv`\n",
    "- Update the `model_name` variable in the Configuration cell to analyze different models\n",
    "\n",
    "**File Outputs:**\n",
    "All files are saved to the `consistency_extraction` subdirectory:\n",
    "1. CSV files with similarity scores (full and summary)\n",
    "2. Visualization PNGs (distributions, boxplot, correlation heatmap)\n",
    "\n",
    "**Output Structure:**\n",
    "```\n",
    "/content/drive/MyDrive/aisysevalteam-data/\n",
    "â””â”€â”€ consistency_extraction/\n",
    "    â”œâ”€â”€ consolidated_similarity_scores_google_gemini2.5pro.csv\n",
    "    â”œâ”€â”€ similarity_scores_summary_google_gemini2.5pro.csv\n",
    "    â”œâ”€â”€ similarity_distributions_google_gemini2.5pro.png\n",
    "    â”œâ”€â”€ consistency_by_grade_google_gemini2.5pro.png\n",
    "    â””â”€â”€ correlation_heatmap_google_gemini2.5pro.png\n",
    "```\n",
    "\n",
    "**To analyze a different model:**\n",
    "Simply change the `model_name` variable in the Configuration cell:\n",
    "```python\n",
    "model_name = 'anthropic/claude-sonnet-4'\n",
    "```\n",
    "\n",
    "**To analyze all models:**\n",
    "Comment out the model-specific path and use the base directory:\n",
    "```python\n",
    "# base_dir = Path(file_path) / model_name\n",
    "base_dir = Path(file_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "**Configuration:**\n",
    "- All outputs are saved to Google Drive: `/content/drive/MyDrive/aisysevalteam-data/`\n",
    "- Files are named with the model name to avoid conflicts: `consolidated_similarity_scores_google_gemini2.5pro.csv`\n",
    "- Update the `model_name` variable in the Configuration cell to analyze different models\n",
    "\n",
    "**File Outputs:**\n",
    "1. CSV files with similarity scores (full and summary)\n",
    "2. Visualization PNGs (distributions, boxplot, correlation heatmap)\n",
    "\n",
    "**To analyze a different model:**\n",
    "Simply change the `model_name` variable in the Configuration cell:\n",
    "```python\n",
    "model_name = 'anthropic/claude-sonnet-4'\n",
    "```\n",
    "\n",
    "**To analyze all models:**\n",
    "Comment out the model-specific path and use the base directory:\n",
    "```python\n",
    "# base_dir = Path(file_path) / model_name\n",
    "base_dir = Path(file_path)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
