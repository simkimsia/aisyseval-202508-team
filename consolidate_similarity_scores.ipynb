{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Consolidate Similarity Scores Across Multiple Runs\n\nThis notebook extracts and consolidates similarity scores from multiple run_summary.json files stored in Google Drive.\n\n**Key Points:**\n- Similarity scores are at the **instance level** (not instance-run level)\n- They compare multiple runs of the same instance\n- Located in: `multi_run_info.consistency_metrics.instance_reports`\n\n**Setup:**\n- Designed for Google Colab\n- Mounts Google Drive to access data files\n- Outputs saved to: `/content/drive/MyDrive/aisysevalteam-data/`\n\n**Quick Start:**\n1. Run all cells in order\n2. Adjust `model_name` in Configuration cell for different models\n3. CSV files and visualizations will be saved to Google Drive"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nimport json\nimport pandas as pd\nfrom pathlib import Path\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration\n\n**Update these variables to match your Google Drive structure:**\n- `file_path`: Base directory in Google Drive where results are stored\n- `model_name`: Specific model directory to analyze (e.g., 'google/gemini2.5pro')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nfile_path = '/content/drive/MyDrive/aisysevalteam-data/'\nmodel_name = 'google/gemini2.5pro'\n\n# Construct base directory path\nbase_dir = Path(file_path) / model_name\n\n# Alternative: Search all models in the base directory\n# base_dir = Path(file_path)\n\nprint(f\"Base data path: {file_path}\")\nprint(f\"Model name: {model_name}\")\nprint(f\"Searching in: {base_dir}\")\nprint(f\"Directory exists: {base_dir.exists()}\")\n\n# If directory doesn't exist, list what's available\nif not base_dir.exists():\n    print(f\"\\nâš ï¸ Directory not found!\")\n    print(f\"\\nAvailable directories in {file_path}:\")\n    base_path = Path(file_path)\n    if base_path.exists():\n        for item in sorted(base_path.iterdir()):\n            if item.is_dir():\n                print(f\"  - {item.name}\")\n    else:\n        print(f\"âš ï¸ Base path {file_path} does not exist!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find All run_summary.json Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all run_summary.json files\n",
    "run_summary_files = list(base_dir.glob(\"**/run_summary.json\"))\n",
    "\n",
    "print(f\"Found {len(run_summary_files)} run_summary.json files:\\n\")\n",
    "for f in sorted(run_summary_files):\n",
    "    print(f\"  {f.relative_to(base_dir.parent)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Similarity Scores by Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Dictionary to store all data by instance_id\n# Structure: {instance_id: [record1, record2, ...]}\ninstances_data = defaultdict(list)\n\nfor summary_file in run_summary_files:\n    try:\n        with open(summary_file, 'r') as f:\n            data = json.load(f)\n        \n        # Extract run metadata\n        run_info = data.get('run_info', {})\n        model_name_from_file = run_info.get('model_name', 'unknown')\n        timestamp = run_info.get('timestamp', 'unknown')\n        num_runs_configured = run_info.get('num_runs_configured', 1)\n        \n        # Calculate relative path for run_dir\n        try:\n            run_dir_rel = str(summary_file.parent.relative_to(base_dir))\n        except ValueError:\n            # If relative_to fails, use the full path\n            run_dir_rel = str(summary_file.parent)\n        \n        # Extract instance reports from multi_run_info\n        multi_run_info = data.get('multi_run_info', {})\n        consistency_metrics = multi_run_info.get('consistency_metrics', {})\n        \n        # Handle both completed consistency checks and skipped ones\n        if isinstance(consistency_metrics, dict):\n            instance_reports = consistency_metrics.get('instance_reports', [])\n        else:\n            instance_reports = []\n        \n        # Process each instance report\n        for report in instance_reports:\n            instance_id = report.get('instance_id')\n            if not instance_id:\n                continue\n            \n            # Extract patch consistency metrics\n            patch_consistency = report.get('patch_consistency', {})\n            evaluation_consistency = report.get('evaluation_consistency', {})\n            security_consistency = report.get('security_consistency', {})\n            \n            # Create record\n            record = {\n                'instance_id': instance_id,\n                'model_name': model_name_from_file,\n                'timestamp': timestamp,\n                'run_dir': run_dir_rel,\n                'num_runs': report.get('num_runs', 1),\n                'num_runs_configured': num_runs_configured,\n                \n                # Similarity scores (0.0-1.0)\n                'avg_ast_similarity': patch_consistency.get('avg_ast_similarity'),\n                'avg_text_similarity': patch_consistency.get('avg_text_similarity'),\n                'avg_hybrid_similarity': patch_consistency.get('avg_hybrid_similarity'),\n                \n                # Other patch metrics\n                'exact_match_rate': patch_consistency.get('exact_match_rate'),\n                'num_unique_patches': patch_consistency.get('num_unique_patches'),\n                'confidence_percent': patch_consistency.get('confidence_percent'),\n                'normalized_confidence_percent': patch_consistency.get('normalized_confidence_percent'),\n                'agreement_percent': patch_consistency.get('agreement_percent'),\n                'line_count_variance': patch_consistency.get('line_count_variance'),\n                \n                # Consistency scores\n                'overall_consistency_score': report.get('overall_consistency_score'),\n                'consistency_grade': report.get('consistency_grade'),\n                \n                # Component scores\n                'patch_score': report.get('component_scores', {}).get('patch_score'),\n                'evaluation_score': report.get('component_scores', {}).get('evaluation_score'),\n                'security_score': report.get('component_scores', {}).get('security_score'),\n                \n                # Evaluation consistency\n                'all_resolved': evaluation_consistency.get('all_resolved'),\n                'all_failed': evaluation_consistency.get('all_failed'),\n                'resolution_rate': evaluation_consistency.get('resolution_rate'),\n                'num_evaluations': evaluation_consistency.get('num_evaluations'),\n                \n                # Security consistency\n                'risk_level_mode': security_consistency.get('risk_level_mode'),\n                'security_score_variance': security_consistency.get('score_variance'),\n                'num_scans': security_consistency.get('num_scans'),\n                'avg_security_score': security_consistency.get('avg_score'),\n            }\n            \n            instances_data[instance_id].append(record)\n    \n    except Exception as e:\n        print(f\"Error processing {summary_file}: {e}\")\n        continue\n\nprint(f\"\\nExtracted data for {len(instances_data)} unique instances\")\nprint(f\"Total records: {sum(len(records) for records in instances_data.values())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Consolidated DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data into a list of records\n",
    "all_records = []\n",
    "for instance_id, records in instances_data.items():\n",
    "    all_records.extend(records)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_records)\n",
    "\n",
    "# Sort by instance_id and timestamp\n",
    "df = df.sort_values(['instance_id', 'timestamp'])\n",
    "\n",
    "print(f\"Created DataFrame with {len(df)} rows and {len(df.columns)} columns\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Data Dictionary\n\nThe extracted dataset includes the following fields:\n\n### Metadata\n- **instance_id**: SWE-bench instance identifier\n- **model_name**: AI model used\n- **timestamp**: Run timestamp\n- **run_dir**: Directory containing the run data\n- **num_runs**: Actual number of runs performed\n- **num_runs_configured**: Number of runs configured\n\n### Similarity Scores (0.0-1.0, higher = more similar)\n- **avg_ast_similarity**: Average structural code similarity (AST-based)\n- **avg_text_similarity**: Average surface-level text similarity\n- **avg_hybrid_similarity**: Weighted combination of AST and text similarity\n\n### Patch Consistency Metrics\n- **exact_match_rate**: Fraction of patches that are byte-for-byte identical\n- **num_unique_patches**: Number of distinct patches generated\n- **confidence_percent**: Main consistency metric (0-100)\n- **normalized_confidence_percent**: Rescaled confidence metric (0-100)\n- **agreement_percent**: Percentage of pairs above similarity threshold (0-100)\n- **line_count_variance**: Variation in patch sizes\n\n### Overall Consistency\n- **overall_consistency_score**: Weighted score combining all metrics (0-100)\n- **consistency_grade**: EXCELLENT / GOOD / FAIR / POOR\n\n### Component Scores (0-100)\n- **patch_score**: Patch consistency component (40% weight)\n- **evaluation_score**: Evaluation consistency component (40% weight)\n- **security_score**: Security consistency component (20% weight)\n\n### Evaluation Consistency\n- **all_resolved**: Whether all runs resolved the issue\n- **all_failed**: Whether all runs failed the evaluation\n- **resolution_rate**: Fraction of runs that passed (0.0-1.0)\n- **num_evaluations**: Number of evaluations performed\n\n### Security Consistency\n- **risk_level_mode**: Most common security risk level (NONE/LOW/MEDIUM/HIGH)\n- **security_score_variance**: Variance in security scores\n- **num_scans**: Number of security scans performed\n- **avg_security_score**: Average security risk score across runs",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SIMILARITY SCORES SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "similarity_cols = ['avg_ast_similarity', 'avg_text_similarity', 'avg_hybrid_similarity']\n",
    "print(\"\\nSimilarity Scores Statistics (0.0 = very different, 1.0 = identical):\")\n",
    "print(df[similarity_cols].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONSISTENCY GRADES DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "print(df['consistency_grade'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONSISTENCY SCORES BY GRADE\")\n",
    "print(\"=\" * 80)\n",
    "print(df.groupby('consistency_grade')['overall_consistency_score'].agg(['mean', 'min', 'max', 'count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Specific Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show instances with multiple runs across different timestamps\n",
    "instance_counts = df.groupby('instance_id').size().sort_values(ascending=False)\n",
    "print(\"Instances with multiple consistency reports:\")\n",
    "print(instance_counts[instance_counts > 1])\n",
    "\n",
    "# Pick an instance to examine in detail\n",
    "if len(instance_counts) > 0:\n",
    "    example_instance = instance_counts.index[0]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Detailed view of: {example_instance}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    cols_to_show = [\n",
    "        'timestamp', 'num_runs', \n",
    "        'avg_ast_similarity', 'avg_text_similarity', 'avg_hybrid_similarity',\n",
    "        'overall_consistency_score', 'consistency_grade'\n",
    "    ]\n",
    "    \n",
    "    display(df[df['instance_id'] == example_instance][cols_to_show])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by Consistency Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show instances with EXCELLENT consistency\n",
    "excellent = df[df['consistency_grade'] == 'EXCELLENT']\n",
    "print(f\"Instances with EXCELLENT consistency: {len(excellent)}\")\n",
    "if len(excellent) > 0:\n",
    "    display(excellent[['instance_id', 'timestamp', 'avg_ast_similarity', \n",
    "                       'avg_text_similarity', 'overall_consistency_score']].head(10))\n",
    "\n",
    "# Show instances with POOR consistency\n",
    "poor = df[df['consistency_grade'] == 'POOR']\n",
    "print(f\"\\nInstances with POOR consistency: {len(poor)}\")\n",
    "if len(poor) > 0:\n",
    "    display(poor[['instance_id', 'timestamp', 'avg_ast_similarity', \n",
    "                  'avg_text_similarity', 'overall_consistency_score']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export full dataset\noutput_file = Path(file_path) / f'consolidated_similarity_scores_{model_name.replace(\"/\", \"_\")}.csv'\ndf.to_csv(output_file, index=False)\nprint(f\"âœ… Exported full dataset to: {output_file}\")\n\n# Export summary by instance (latest run only)\nsummary_file = Path(file_path) / f'similarity_scores_summary_{model_name.replace(\"/\", \"_\")}.csv'\nlatest_df = df.sort_values('timestamp').groupby('instance_id').last().reset_index()\nlatest_df.to_csv(summary_file, index=False)\nprint(f\"âœ… Exported summary (latest run per instance) to: {summary_file}\")\n\nprint(f\"\\nðŸ“Š Full dataset: {len(df)} records\")\nprint(f\"ðŸ“Š Summary dataset: {len(latest_df)} records (one per instance)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Plot 1: Distribution of similarity scores\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\ndf['avg_ast_similarity'].hist(bins=20, ax=axes[0], edgecolor='black')\naxes[0].set_title('AST Similarity Distribution')\naxes[0].set_xlabel('Similarity Score')\naxes[0].set_ylabel('Frequency')\n\ndf['avg_text_similarity'].hist(bins=20, ax=axes[1], edgecolor='black')\naxes[1].set_title('Text Similarity Distribution')\naxes[1].set_xlabel('Similarity Score')\n\ndf['avg_hybrid_similarity'].hist(bins=20, ax=axes[2], edgecolor='black')\naxes[2].set_title('Hybrid Similarity Distribution')\naxes[2].set_xlabel('Similarity Score')\n\nplt.tight_layout()\nplot1_file = Path(file_path) / f'similarity_distributions_{model_name.replace(\"/\", \"_\")}.png'\nplt.savefig(plot1_file, dpi=150, bbox_inches='tight')\nplt.show()\n\n# Plot 2: Consistency scores by grade\nfig, ax = plt.subplots(figsize=(10, 6))\ndf.boxplot(column='overall_consistency_score', by='consistency_grade', ax=ax)\nplt.title('Consistency Scores by Grade')\nplt.suptitle('')  # Remove default title\nplt.xlabel('Consistency Grade')\nplt.ylabel('Overall Consistency Score')\nplot2_file = Path(file_path) / f'consistency_by_grade_{model_name.replace(\"/\", \"_\")}.png'\nplt.savefig(plot2_file, dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"âœ… Saved visualizations:\")\nprint(f\"  - {plot1_file}\")\nprint(f\"  - {plot2_file}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze correlation between different metrics\ncorrelation_cols = [\n    'avg_ast_similarity', 'avg_text_similarity', 'avg_hybrid_similarity',\n    'exact_match_rate', 'confidence_percent', 'overall_consistency_score'\n]\n\ncorrelation_matrix = df[correlation_cols].corr()\n\n# Plot correlation heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n            center=0, square=True, linewidths=1)\nplt.title('Correlation Between Consistency Metrics')\nplt.tight_layout()\nheatmap_file = Path(file_path) / f'correlation_heatmap_{model_name.replace(\"/\", \"_\")}.png'\nplt.savefig(heatmap_file, dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"âœ… Saved {heatmap_file}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Notes\n\n**Configuration:**\n- All outputs are saved to Google Drive: `/content/drive/MyDrive/aisysevalteam-data/`\n- Files are named with the model name to avoid conflicts: `consolidated_similarity_scores_google_gemini2.5pro.csv`\n- Update the `model_name` variable in the Configuration cell to analyze different models\n\n**File Outputs:**\n1. CSV files with similarity scores (full and summary)\n2. Visualization PNGs (distributions, boxplot, correlation heatmap)\n\n**To analyze a different model:**\nSimply change the `model_name` variable in the Configuration cell:\n```python\nmodel_name = 'anthropic/claude-sonnet-4'\n```\n\n**To analyze all models:**\nComment out the model-specific path and use the base directory:\n```python\n# base_dir = Path(file_path) / model_name\nbase_dir = Path(file_path)\n```",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}